{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Text_Classification_With_ML.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bNmF35uAvEi2"
      },
      "source": [
        "# Text Classification is one model of supervised machine learning task with a labelled dataset containing text documents and their labels is used for train a classifier.\n",
        "\n",
        "# Steps : -\n",
        "\n",
        "# 1-Dataset Preparation: \n",
        "Dataset Preparation step which includes the process of loading a dataset and performing basic cleaning and pre-processing. The dataset is then splitted into train and test sets.\n",
        "\n",
        "\n",
        "# 2-Feature Engineering: \n",
        "Feature Engineering : the raw dataset is transformed into some flat features which can be used in a machine learning model. This step also includes the process of creating new features from the existing data.\n",
        "\n",
        "\n",
        "# 3.-Model Training: \n",
        "Model Building step in which a machine learning model is trained on a labelled dataset.\n",
        "\n",
        "\n",
        "# 4-Improve Performance of Text Classifier: \n",
        "we will use different ways to improve the performance of text classifiers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BIn3kxT2vEi3",
        "outputId": "403ef2e8-4b3c-4de7-9488-07ef98c1367f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# libraries for dataset preparation, feature engineering, model training \n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "from keras.layers import Input\n",
        "import tensorflow\n",
        "from tensorflow.keras.layers import Input\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style=\"darkgrid\")\n",
        "sns.set(font_scale=1.3)\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/mazensalama/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7RvD5zhg1giS",
        "colab": {}
      },
      "source": [
        "#!pip3 install --ignore-installed --upgrade https://github.com/lakshayg/tensorflow-build/releases/download/tf1.13.1-ubuntu16.04-py3/tensorflow-1.13.1-cp37-cp37m-macosx_10_9_x86_64.whl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "50Ap2DJCvEi6"
      },
      "source": [
        "# 1-Dataset preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0tEo1-j3vEi6",
        "colab": {}
      },
      "source": [
        "#loading the data set\n",
        "df = pandas.read_csv('AtrialFibrillationOA-InterimExport_DATA_2019-09-18_0653 (1).csv', delimiter = \"|\")\n",
        "df = df.reindex(numpy.random.permutation(df.index))\n",
        "df = df[['aggregated_phrase', 'oac_class']]\n",
        "\n",
        "# filter out rows ina . dataframe with column oac_class values NA/NAN\n",
        "df = df[df.oac_class.notnull()]\n",
        "# drop unwanted \n",
        "#df = df[df.oac_class != 0]\n",
        "#df = df[df.oac_class != 3]\n",
        "#df = df[df.oac_class != 4]\n",
        "#df = df[df.oac_class != 5]\n",
        "#remove duplicates phrases\n",
        "df = df.drop_duplicates(subset='aggregated_phrase', keep=\"last\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-anlOU_lvEi8",
        "outputId": "08aad294-eab4-4dba-e98c-7ec3824faee2",
        "colab": {}
      },
      "source": [
        "#showing counts of each value for lables\n",
        "df.oac_class.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0    625\n",
              "3.0     42\n",
              "1.0     39\n",
              "5.0     30\n",
              "0.0     14\n",
              "4.0     13\n",
              "Name: oac_class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fd1ZwjHyvEjC",
        "outputId": "92ec5c54-6519-4049-a104-c1d549f72f30",
        "colab": {}
      },
      "source": [
        "#Visual\n",
        "sns.factorplot(x=\"oac_class\", data=df, kind=\"count\", size=6, aspect=1.5, palette=\"PuBuGn_d\")\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/site-packages/seaborn/categorical.py:3666: UserWarning: The `factorplot` function has been renamed to `catplot`. The original name will be removed in a future release. Please update your code. Note that the default `kind` in `factorplot` (`'point'`) has changed `'strip'` in `catplot`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.7/site-packages/seaborn/categorical.py:3672: UserWarning: The `size` paramter has been renamed to `height`; please update your code.\n",
            "  warnings.warn(msg, UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kb9MXKMp6Y53",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now aggregate \n",
        "df.loc[df.oac_class == 0, 'oac_class1'] = '0' \n",
        "df.loc[df.oac_class == 1, 'oac_class1'] = '1' \n",
        "df.loc[df.oac_class == 2, 'oac_class1'] = '1'\n",
        "df.loc[df.oac_class == 3, 'oac_class1'] = '0' \n",
        "df.loc[df.oac_class == 4, 'oac_class1'] = '0'\n",
        "df.loc[df.oac_class == 5, 'oac_class1'] = '0' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-TNNIAhuvEjG",
        "outputId": "8f584ec2-3c56-4f27-9e4d-62951458ba92",
        "colab": {}
      },
      "source": [
        "#Visually the new aggregated column\n",
        "sns.factorplot(x=\"oac_class1\", data=df, kind=\"count\", size=6, aspect=1.5, palette=\"PuBuGn_d\")\n",
        "plt.show();"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 648x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OpIRPe3pvEjI",
        "outputId": "1570af79-e147-4a78-84ca-25d585a826d3",
        "colab": {}
      },
      "source": [
        "#count the value\n",
        "df.oac_class1.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    664\n",
              "0     99\n",
              "Name: oac_class1, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gGpDL8jcvEjM",
        "outputId": "1d7cd19f-7b47-46c8-e83e-36987bf284c5",
        "colab": {}
      },
      "source": [
        "# Libraries for text preprocessing and loading our list of stop word\n",
        "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn import decomposition, ensemble\n",
        "import pandas, xgboost, numpy, textblob, string\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras import layers, models, optimizers\n",
        "from keras.layers import Input\n",
        "import tensorflow\n",
        "from tensorflow.keras.layers import Input\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style=\"darkgrid\")\n",
        "sns.set(font_scale=1.3)\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "import nltk\n",
        "#nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "#nltk.download('wordnet') \n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "##Creating a list of stop words and adding custom stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "#unpickled_df = pd.read_pickle(\"stop_words500.pkl\")\n",
        "#unpickled_df\n",
        "##Creating a list of custom stopwords\n",
        "new_words = ['and',\n",
        " 'the',\n",
        " 'of',\n",
        " 'to',\n",
        " 'with',\n",
        " 'mg',\n",
        " 'no',\n",
        " 'is',\n",
        " 'in',\n",
        " 'for',\n",
        " 'on',\n",
        " 'patient',\n",
        " 'he',\n",
        " 'was',\n",
        " 'or',\n",
        " 'she',\n",
        " 'history',\n",
        " '10',\n",
        " 'at',\n",
        " 'by',\n",
        " 'has',\n",
        " 'as',\n",
        " 'daily',\n",
        " 'this',\n",
        " '12',\n",
        " 'his',\n",
        " 'not',\n",
        " 'normal',\n",
        " 'her',\n",
        " 'tablet',\n",
        " 'that',\n",
        " 'take',\n",
        " 'pain',\n",
        " '11',\n",
        " 'mouth',\n",
        " '15',\n",
        " 'will',\n",
        " '16',\n",
        " 'left',\n",
        " '17',\n",
        " 'have',\n",
        " 'time',\n",
        " '20',\n",
        " 'right',\n",
        " 'are',\n",
        " '14',\n",
        " 'day',\n",
        " '30',\n",
        " 'md',\n",
        " 'date',\n",
        " 'from',\n",
        " 'had',\n",
        " 'plan',\n",
        " 'oral',\n",
        " 'heart',\n",
        " 'ml',\n",
        " 'tab',\n",
        " 'be',\n",
        " 'up',\n",
        " 'well',\n",
        " 'we',\n",
        " '25',\n",
        " 'last',\n",
        " 'chest',\n",
        " 'prn',\n",
        " 'past',\n",
        " '24',\n",
        " 'dl',\n",
        " 'care',\n",
        " 'negative',\n",
        " 'medications',\n",
        " 'today',\n",
        " 'blood',\n",
        " 'disease',\n",
        " 'pt',\n",
        " 'status',\n",
        " 'an',\n",
        " 'continue',\n",
        " '08',\n",
        " 'po',\n",
        " '09',\n",
        " 'but',\n",
        " 'exam',\n",
        " 'year',\n",
        " 'medical',\n",
        " '40',\n",
        " '07',\n",
        " 'any',\n",
        " 'been',\n",
        " '2015',\n",
        " '2016',\n",
        " '50',\n",
        " '05',\n",
        " 'dr',\n",
        " '06',\n",
        " 'other',\n",
        " 'physical',\n",
        " '100',\n",
        " '03',\n",
        " 'reviewed',\n",
        " 'follow',\n",
        " '13',\n",
        " 'if',\n",
        " 'there',\n",
        " 'per',\n",
        " 'rate',\n",
        " '01',\n",
        " 'old',\n",
        " 'note',\n",
        " 'clinic',\n",
        " 'atrial',\n",
        " '04',\n",
        " '2014',\n",
        " 'times',\n",
        " 'which',\n",
        " '02',\n",
        " 'skin',\n",
        " '2017',\n",
        " 'present',\n",
        " 'visit',\n",
        " 'every',\n",
        " 'given',\n",
        " 'mmol',\n",
        " 'also',\n",
        " 'hours',\n",
        " 'medication',\n",
        " 'units',\n",
        " 'it',\n",
        " '18',\n",
        " 'range',\n",
        " 'use',\n",
        " 'sodium',\n",
        " 'days',\n",
        " 'yes',\n",
        " 'acute',\n",
        " 'assessment',\n",
        " 'were',\n",
        " 'review',\n",
        " 'discharge',\n",
        " 'family',\n",
        " 'kg',\n",
        " 'needed',\n",
        " 'without',\n",
        " 'home',\n",
        " 'edema',\n",
        " 'after',\n",
        " '36',\n",
        " 'bid',\n",
        " '26',\n",
        " 'current',\n",
        " 'value',\n",
        " 'results',\n",
        " 'symptoms',\n",
        " 'denies',\n",
        " 'therapy',\n",
        " 'dose',\n",
        " 'pulmonary',\n",
        " 'chronic',\n",
        " 'surgery',\n",
        " '00',\n",
        " 'pressure',\n",
        " '22',\n",
        " 'lower',\n",
        " 'does',\n",
        " 'back',\n",
        " '21',\n",
        " 'hospital',\n",
        " 'hypertension',\n",
        " 'procedure',\n",
        " 'neck',\n",
        " '60',\n",
        " 'noted',\n",
        " '19',\n",
        " 'total',\n",
        " 'who',\n",
        " '23',\n",
        " 'post',\n",
        " 'systems',\n",
        " 'fibrillation',\n",
        " 'all',\n",
        " 'ul',\n",
        " 'am',\n",
        " 'due',\n",
        " 'general',\n",
        " 'prior',\n",
        " 'above',\n",
        " '28',\n",
        " 'respiratory',\n",
        " 'none',\n",
        " '27',\n",
        " 'iv',\n",
        " 'diagnosis',\n",
        " 'intravenous',\n",
        " 'chloride',\n",
        " 'you',\n",
        " 'failure',\n",
        " '2013',\n",
        " 'breath',\n",
        " 'social',\n",
        " 'cardiac',\n",
        " '29',\n",
        " 'some',\n",
        " 'capsule',\n",
        " 'weight',\n",
        " '2012',\n",
        " 'diabetes',\n",
        " '90',\n",
        " 'one',\n",
        " 'type',\n",
        " 'mcg',\n",
        " 'potassium',\n",
        " 'ct',\n",
        " 'result',\n",
        " 'surgical',\n",
        " 'since',\n",
        " 'may',\n",
        " 'cardiovascular',\n",
        " 'bp',\n",
        " 'treatment',\n",
        " 'tube',\n",
        " 'discussed',\n",
        " 'regular',\n",
        " 'cancer',\n",
        " 'glucose',\n",
        " 'see',\n",
        " 'when',\n",
        " 'pulse',\n",
        " 'sounds',\n",
        " 'rhythm',\n",
        " 'utah',\n",
        " 'allergies',\n",
        " 'intact',\n",
        " 'than',\n",
        " 'cm',\n",
        " 'significant',\n",
        " 'clear',\n",
        " 'stable',\n",
        " 'reports',\n",
        " 'abdomen',\n",
        " 'examination',\n",
        " 'seen',\n",
        " 'week',\n",
        " 'mild',\n",
        " 'extremities',\n",
        " 'now',\n",
        " 'following',\n",
        " 'years',\n",
        " '2011',\n",
        " 'admission',\n",
        " 'about',\n",
        " 'lab',\n",
        " 'bilateral',\n",
        " 'distress',\n",
        " 'bowel',\n",
        " '31',\n",
        " 'service',\n",
        " 'hr',\n",
        " 'temp',\n",
        " 'taking',\n",
        " 'attending',\n",
        " 'ref',\n",
        " 'provider',\n",
        " 'name',\n",
        " 'encounter',\n",
        " 'recent',\n",
        " 'information',\n",
        " '97',\n",
        " 'new',\n",
        " 'months',\n",
        " 'university',\n",
        " 'over',\n",
        " 'performed',\n",
        " 'male',\n",
        " 'health',\n",
        " 'signs',\n",
        " 'soft',\n",
        " 'artery',\n",
        " 'mmhg',\n",
        " 'changes',\n",
        " 'would',\n",
        " 'your',\n",
        " 'illness',\n",
        " 'weeks',\n",
        " 'sleep',\n",
        " 'currently',\n",
        " 'calcium',\n",
        " 'alert',\n",
        " 'likely',\n",
        " '80',\n",
        " 'renal',\n",
        " 'findings',\n",
        " 'inr',\n",
        " 'insulin',\n",
        " '98',\n",
        " 'abdominal',\n",
        " 'low',\n",
        " 'physician',\n",
        " 'mr',\n",
        " 'did',\n",
        " 'oriented',\n",
        " '500',\n",
        " 'more',\n",
        " 'place',\n",
        " 'once',\n",
        " 'pm',\n",
        " 'final',\n",
        " 'ventricular',\n",
        " 'imaging',\n",
        " 'min',\n",
        " '81',\n",
        " 'him',\n",
        " 'labs',\n",
        " 'non',\n",
        " 'collection',\n",
        " 'upper',\n",
        " 'then',\n",
        " 'increased',\n",
        " 'bilaterally',\n",
        " 'started',\n",
        " 'female',\n",
        " 'do',\n",
        " 'function',\n",
        " 'evidence',\n",
        " 'coronary',\n",
        " 'lb',\n",
        " 'warfarin',\n",
        " 'positive',\n",
        " 'used',\n",
        " 'diet',\n",
        " 'head',\n",
        " 'aspirin',\n",
        " 'hcc',\n",
        " 'multiple',\n",
        " 'during',\n",
        " 'showed',\n",
        " 'primary',\n",
        " 'change',\n",
        " 'transplant',\n",
        " 'resp',\n",
        " 'active',\n",
        " 'morning',\n",
        " 'creatinine',\n",
        " 'known',\n",
        " 'agree',\n",
        " 'problem',\n",
        " 'age',\n",
        " 'placement',\n",
        " '70',\n",
        " 'lung',\n",
        " 'spo2',\n",
        " 'vitamin',\n",
        " 'sig',\n",
        " 'nausea',\n",
        " 'two',\n",
        " '37',\n",
        " 'however',\n",
        " 'cardiology',\n",
        " 'call',\n",
        " 'file',\n",
        " 'alcohol',\n",
        " 'valve',\n",
        " 'high',\n",
        " 'states',\n",
        " 'management',\n",
        " 'decreased',\n",
        " 'appears',\n",
        " 'good',\n",
        " 'monitor',\n",
        " 'very',\n",
        " 'level',\n",
        " '2010',\n",
        " 'oxygen',\n",
        " 'gi',\n",
        " 'shortness',\n",
        " 'eyes',\n",
        " 'extremity',\n",
        " 'start',\n",
        " 'these',\n",
        " 'further',\n",
        " 'ago',\n",
        " 'knee',\n",
        " '35',\n",
        " 'vital',\n",
        " 'personally',\n",
        " '75',\n",
        " 'evaluation',\n",
        " 'found',\n",
        " 'mrn',\n",
        " 'injection',\n",
        " 'severe',\n",
        " 'minutes',\n",
        " 'infection',\n",
        " 'systolic',\n",
        " 'urine',\n",
        " 'stroke',\n",
        " 'bun',\n",
        " 'fluid',\n",
        " 'drug',\n",
        " 'can',\n",
        " 'bedtime',\n",
        " 'before',\n",
        " 'oz',\n",
        " 'bleeding',\n",
        " '000',\n",
        " 'tenderness',\n",
        " '200',\n",
        " 'outpatient',\n",
        " 'output',\n",
        " 'data',\n",
        " 'lungs',\n",
        " 'ms',\n",
        " 'out',\n",
        " 'small',\n",
        " 'activity',\n",
        " 'coumadin',\n",
        " '72',\n",
        " 'never',\n",
        " 'night',\n",
        " 'admitted',\n",
        " 'risk',\n",
        " 'dry',\n",
        " 'medicine',\n",
        " 'kidney',\n",
        " 'possible',\n",
        " 'resident',\n",
        " 'sinus',\n",
        " 'need',\n",
        " 'wound',\n",
        " 'point',\n",
        " 'full',\n",
        " 'hct',\n",
        " 'elevated',\n",
        " '96',\n",
        " 'comment',\n",
        " 'improved',\n",
        " 'most',\n",
        " '32',\n",
        " 'leg',\n",
        " 'intake',\n",
        " 'swelling',\n",
        " 'hour',\n",
        " 'location',\n",
        " 'eye',\n",
        " 'including',\n",
        " 'done',\n",
        " 'cough',\n",
        " 'moderate',\n",
        " 'loss',\n",
        " 'tablets',\n",
        " 'wbc',\n",
        " 'unit',\n",
        " 'twice',\n",
        " 'heent',\n",
        " 'hgb',\n",
        " 'aortic',\n",
        " '99',\n",
        " 'presents',\n",
        " 'report',\n",
        " 'into',\n",
        " 'problems',\n",
        " 'instructions',\n",
        " 'meq',\n",
        " 'they',\n",
        " 'secondary',\n",
        " 'neurological',\n",
        " 'only',\n",
        " 'tobacco',\n",
        " 'temporal',\n",
        " 'light',\n",
        " 'appropriate',\n",
        " 'metoprolol',\n",
        " 'cell',\n",
        " 'cap',\n",
        " 'acetaminophen',\n",
        " 'heparin',\n",
        " 'mass',\n",
        " 'cath',\n",
        " 'test',\n",
        " 'lasix',\n",
        " 'cont',\n",
        " 'site',\n",
        " 'questions',\n",
        " 'off',\n",
        " 'ii',\n",
        " 'cc',\n",
        " 'lesions',\n",
        " '45',\n",
        " 'less',\n",
        " 'side',\n",
        " 'warm',\n",
        " 'within',\n",
        " 'examined',\n",
        " 'please',\n",
        " 'room',\n",
        " '0700',\n",
        " 'rn',\n",
        " 'infusion',\n",
        " 'next',\n",
        " 'like',\n",
        " 'team']\n",
        "stop_words = stop_words.union(new_words)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "#tokenizer to remove unwanted elements from out data like symbols and numbers\n",
        "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
        "cv = CountVectorizer(lowercase=True,stop_words=stop_words,ngram_range = (1,1),tokenizer = token.tokenize)\n",
        "text_counts= cv.fit_transform(df['aggregated_phrase'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/mazensalama/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9PZca2r6Y6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#More Data Cleaning\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "        text: a string\n",
        "        \n",
        "        return: modified initial string\n",
        "    \"\"\"\n",
        "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
        "    text = text.lower() # lowercase text\n",
        "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
        "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
        "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
        "    return text\n",
        "    \n",
        "df['aggregated_phrase'] = df['aggregated_phrase'].apply(clean_text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G-fgkmOavEjT",
        "colab": {}
      },
      "source": [
        "# split the dataset into training and validation datasets \n",
        "train_x, valid_x, train_y, valid_y = model_selection.train_test_split(df['aggregated_phrase'], df['oac_class1'], test_size=0.3)\n",
        "\n",
        "# label encode the target variable \n",
        "encoder = preprocessing.LabelEncoder()\n",
        "train_y = encoder.fit_transform(train_y)\n",
        "valid_y = encoder.fit_transform(valid_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dWBfq8aavEjV"
      },
      "source": [
        "# 2. Feature Engineering\n",
        "\n",
        "In this step, raw text data will be transformed into feature vectors and the new features will be created using the existing dataset. We will implement the following different ideas in order to obtain relevant features from our dataset.\n",
        "\n",
        "## 1 Count Vectors as features\n",
        "\n",
        "## 2 TF-IDF Vectors as features\n",
        "\n",
        "Word level\n",
        "\n",
        "N-Gram level\n",
        "\n",
        "Character level\n",
        "\n",
        "## 3 Word Embeddings as features\n",
        "\n",
        "## 4 Text / NLP based features\n",
        "\n",
        "## 5 Topic Models as features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EspNg0L6Y6F",
        "colab_type": "text"
      },
      "source": [
        "# Steps of the implementation \n",
        "\n",
        "## 1 Count Vectors as features\n",
        "\n",
        "Count Vector is a matrix notation of the dataset in which:\n",
        "\n",
        "Every row represents a document from the corpus \n",
        "\n",
        "Every column represents a term from the corpus\n",
        "\n",
        "Every cell represents the frequency count of a particular term in a particular document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4mRD80PEvEjW",
        "colab": {}
      },
      "source": [
        "# create a count vectorizer object \n",
        "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')\n",
        "count_vect.fit(df['aggregated_phrase'])\n",
        "\n",
        "# transform the training and validation data using count vectorizer object\n",
        "xtrain_count =  count_vect.transform(train_x)\n",
        "xvalid_count =  count_vect.transform(valid_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h8Gw6DIMvEja"
      },
      "source": [
        "# 2 TF-IDF Vectors as features\n",
        "\n",
        "TF-IDF score represents the relative importance of a term in the document and the entire corpus. \n",
        "\n",
        "TF-IDF score is #composed by two terms: \n",
        "\n",
        "the first computes the normalized Term Frequency (TF), the second term is the Inverse #Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the #number of documents where the specific term appears.\n",
        "\n",
        "TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
        "\n",
        "IDF(t) = log_e(Total number of documents / Number of documents with term t in it)\n",
        "\n",
        "TF-IDF Vectors can be generated at different levels of input tokens (words, characters, n-grams)\n",
        "\n",
        "#a. Word Level TF-IDF : \n",
        "Matrix representing tf-idf scores of every term in different documents\n",
        "    \n",
        "#b. N-gram Level TF-IDF :\n",
        " N-grams are the combination of N terms together. This Matrix representing tf-idf scores of N-grams\n",
        "    \n",
        "#c. Character Level TF-IDF : \n",
        "Matrix representing tf-idf scores of character level n-grams in the corpus\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zS7pVi-6vEjb",
        "colab": {}
      },
      "source": [
        "# word level tf-idf\n",
        "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
        "tfidf_vect.fit(df['aggregated_phrase'])\n",
        "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
        "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
        "\n",
        "# ngram level tf-idf \n",
        "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram.fit(df['aggregated_phrase'])\n",
        "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
        "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
        "\n",
        "# characters level tf-idf\n",
        "tfidf_vect_ngram_chars = TfidfVectorizer(analyzer='char', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
        "tfidf_vect_ngram_chars.fit(df['aggregated_phrase'])\n",
        "xtrain_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(train_x) \n",
        "xvalid_tfidf_ngram_chars =  tfidf_vect_ngram_chars.transform(valid_x) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OPazQ2Z0vEjd"
      },
      "source": [
        "#2.3 Word Embeddings\n",
        "\n",
        "A word embedding is a form of representing words and documents using a dense vector representation. The position of #a word within the vector space is learned from text and is based on the words that surround the word when it is #used. \n",
        "\n",
        "Word embeddings can be trained using the input corpus itself or can be generated using pre-trained word embeddings #such as Glove, FastText, and Word2Vec. Any one of them can be downloaded and used as transfer learning. \n",
        "Following snnipet shows how to use pre-trained word embeddings in the model.  steps:\n",
        "\n",
        "#1 Loading the pretrained word embeddings\n",
        "\n",
        "#2 Creating a tokenizer object\n",
        "\n",
        "#3 Transforming text documents to sequence of tokens and pad them\n",
        "\n",
        "#4 Create a mapping of token and their respective embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KlsorWKjvEjd",
        "colab": {}
      },
      "source": [
        "# load the pre-trained word-embedding vectors \n",
        "#300d-1M.vec\n",
        "embeddings_index = {}\n",
        "for i, line in enumerate(open('300d-1M.vec')):\n",
        "    values = line.split()\n",
        "    embeddings_index[values[0]] = numpy.asarray(values[1:], dtype='float32')\n",
        "\n",
        "# create a tokenizer \n",
        "token = text.Tokenizer()\n",
        "token.fit_on_texts(df['aggregated_phrase'])\n",
        "word_index = token.word_index\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "train_seq_x = sequence.pad_sequences(token.texts_to_sequences(train_x), maxlen=70)\n",
        "valid_seq_x = sequence.pad_sequences(token.texts_to_sequences(valid_x), maxlen=70)\n",
        "\n",
        "# create token-embedding mapping\n",
        "embedding_matrix = numpy.zeros((len(word_index) + 1, 300))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bl2AGsuKvEjf"
      },
      "source": [
        "#2.4 Text / NLP based features\n",
        "\n",
        "A number of extra text based features can also be created which sometimes are helpful for improving text #classification models. Some examples are:\n",
        "\n",
        "#Word Count of the documents – total number of words in the documents\n",
        "\n",
        "#Character Count of the documents – total number of characters in the documents\n",
        "\n",
        "#Average Word Density of the documents – average length of the words used in the documents\n",
        "\n",
        "#Puncutation Count in the Complete Essay – total number of punctuation marks in the documents\n",
        "\n",
        "#Upper Case Count in the Complete Essay – total number of upper count words in the documents\n",
        "\n",
        "#Title Word Count in the Complete Essay – total number of proper case (title) words in the documents\n",
        "\n",
        "#Frequency distribution of Part of Speech Tags:\n",
        "\n",
        "#1 Noun Count\n",
        "\n",
        "#2 Verb Count\n",
        "\n",
        "#3 Adjective Count\n",
        "\n",
        "#4 Adverb Count\n",
        "\n",
        "#5 Pronoun Count\n",
        "\n",
        "#These features are highly experimental ones and should be used according to the problem statement only.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q0WLoaxovEjg",
        "colab": {}
      },
      "source": [
        "#df['char_count'] = df['aggregated_phrase'].apply(len)\n",
        "#df['word_count'] = df['aggregated_phrase'].apply(lambda x: len(x.split()))\n",
        "#df['word_density'] = df['char_count'] / (df['word_count']+1)\n",
        "#df['punctuation_count'] = df['aggregated_phrase'].apply(lambda x: len(\"\".join(_ for _ in x if _ in string.punctuation))) \n",
        "#df['title_word_count'] = df['aggregated_phrase'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
        "#df['upper_case_word_count'] = df['aggregated_phrase'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uuIU8NrVvEjh",
        "colab": {}
      },
      "source": [
        "pos_family = {\n",
        "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
        "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
        "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
        "    'adj' :  ['JJ','JJR','JJS'],\n",
        "    'adv' : ['RB','RBR','RBS','WRB']\n",
        "}\n",
        "\n",
        "# function to check and get the part of speech tag count of a words in a given sentence\n",
        "def check_pos_tag(x, flag):\n",
        "    cnt = 0\n",
        "    try:\n",
        "        wiki = textblob.TextBlob(x)\n",
        "        for tup in wiki.tags:\n",
        "            ppo = list(tup)[1]\n",
        "            if ppo in pos_family[flag]:\n",
        "                cnt += 1\n",
        "    except:\n",
        "        pass\n",
        "    return cnt\n",
        "\n",
        "df['noun_count'] = df['aggregated_phrase'].apply(lambda x: check_pos_tag(x, 'noun'))\n",
        "df['verb_count'] = df['aggregated_phrase'].apply(lambda x: check_pos_tag(x, 'verb'))\n",
        "df['adj_count'] = df['aggregated_phrase'].apply(lambda x: check_pos_tag(x, 'adj'))\n",
        "df['adv_count'] = df['aggregated_phrase'].apply(lambda x: check_pos_tag(x, 'adv'))\n",
        "df['pron_count'] = df['aggregated_phrase'].apply(lambda x: check_pos_tag(x, 'pron'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LhoIRvaFvEjk"
      },
      "source": [
        "#2.5 Topic Models as features\n",
        "\n",
        "Topic Modelling is a technique to identify the groups of words (called a topic) from a collection of documents that #contains best information in the collection. I have used Latent Dirichlet Allocation for generating Topic Modelling #Features. LDA is an iterative model which starts from a fixed number of topics. Each topic is represented as a #distribution over words, and each document is then represented as a distribution over topics. Although the tokens #themselves are meaningless, the probability distributions over words provided by the topics provide a sense of the #different ideas contained in the documents. One can read more about topic modelling here\n",
        "\n",
        "#Lets see its implementation:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9E3Q86JOvEjl",
        "colab": {}
      },
      "source": [
        "# train a LDA Model\n",
        "lda_model = decomposition.LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20)\n",
        "X_topics = lda_model.fit_transform(xtrain_count)\n",
        "topic_word = lda_model.components_ \n",
        "vocab = count_vect.get_feature_names()\n",
        "\n",
        "# view the topic models\n",
        "n_top_words = 10\n",
        "topic_summaries = []\n",
        "for i, topic_dist in enumerate(topic_word):\n",
        "    topic_words = numpy.array(vocab)[numpy.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
        "    topic_summaries.append(' '.join(topic_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-SXntLZ4vEjo"
      },
      "source": [
        "#3. Model Building\n",
        "\n",
        "The final step in the text classification framework is to train a classifier using the features created in the #previous step. There are many different choices of machine learning models which can be used to train a final model. #We will implement following different classifiers for this purpose:\n",
        "\n",
        "#Naive Bayes Classifier\n",
        "\n",
        "#Linear Classifier\n",
        "\n",
        "#Support Vector Machine\n",
        "\n",
        "#Bagging Models\n",
        "\n",
        "#Boosting Models\n",
        "\n",
        "#Shallow Neural Networks\n",
        "\n",
        "#Deep Neural Networks\n",
        "\n",
        "#Convolutional Neural Network (CNN)\n",
        "\n",
        "#Long Short Term Modelr (LSTM)\n",
        "\n",
        "#Gated Recurrent Unit (GRU)\n",
        "\n",
        "#Bidirectional RNN\n",
        "\n",
        "#Recurrent Convolutional Neural Network (RCNN)\n",
        "\n",
        "#Other Variants of Deep Neural Networks\n",
        "\n",
        "#Lets implement these models and understand their details. \n",
        "\n",
        "\n",
        "The following function is a utility function which can be #used to train a model. It accepts the classifier, feature_vector of training data, labels of training data and #feature vectors of valid data as inputs. Using these inputs, the model is trained and accuracy score is computed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JOGlbTEYvEjo",
        "colab": {}
      },
      "source": [
        "def train_model(classifier, feature_vector_train, label, feature_vector_valid, is_neural_net=False):\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    predictions = classifier.predict(feature_vector_valid)\n",
        "    \n",
        "    if is_neural_net:\n",
        "        predictions = predictions.argmax(axis=-1)\n",
        "    \n",
        "    return metrics.accuracy_score(predictions, valid_y)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrTOT8Y-6Y6Z",
        "colab_type": "text"
      },
      "source": [
        "#### Function to Save Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctijvpGC6Y6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_models(classifier,feature_vector_train,label,model_filename):\n",
        "    classifier.fit(feature_vector_train,label)\n",
        "    save_model_file = '{}_model.pkl'.format(model_filename)\n",
        "    joblib.dump(classifier, save_model_file)\n",
        "    print(\"Saved model as {}\".format(save_model_file))\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8TlZWTKGzBsm"
      },
      "source": [
        "#3.1 Naive Bayes\n",
        "Implementing a naive bayes model using sklearn implementation with different features\n",
        "\n",
        "Naive Bayes is a classification technique based on Bayes’ Theorem with an assumption of independence among #predictors. A Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to #the presence of any other feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bcW_1UgMzQpb",
        "outputId": "ce073e16-41e0-4fd7-ca99-99675c8c87a1",
        "colab": {}
      },
      "source": [
        "# Naive Bayes on Count Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_count, train_y, xvalid_count)\n",
        "print (\"NB, Count Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Word Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print (\"NB, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print (\"NB, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Naive Bayes on Character Level TF IDF Vectors\n",
        "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print (\"NB, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NB, Count Vectors:  0.8864628820960698\n",
            "NB, WordLevel TF-IDF:  0.8733624454148472\n",
            "NB, N-Gram Vectors:  0.8864628820960698\n",
            "NB, CharLevel Vectors:  0.8733624454148472\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lMSIAfftzU0_"
      },
      "source": [
        "#3.2 Linear Classifier (Logistic Regression)\n",
        "\n",
        "Implementing a Linear Classifier (Logistic Regression)\n",
        "\n",
        "Logistic regression measures the relationship between the categorical dependent variable and one or more independent #variables by estimating probabilities using a logistic/sigmoid function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eZK8LaYJzSf7",
        "outputId": "7d3275aa-340a-4db4-fe71-2f93b26a2de4",
        "colab": {}
      },
      "source": [
        "# Linear Classifier on Count Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count)\n",
        "print (\"LR, Count Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Word Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print (\"LR, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print (\"LR, N-Gram Vectors: \", accuracy)\n",
        "\n",
        "# Linear Classifier on Character Level TF IDF Vectors\n",
        "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y, xvalid_tfidf_ngram_chars)\n",
        "print (\"LR, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LR, Count Vectors:  0.9126637554585153\n",
            "LR, WordLevel TF-IDF:  0.8777292576419214\n",
            "LR, N-Gram Vectors:  0.8733624454148472\n",
            "LR, CharLevel Vectors:  0.8733624454148472\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8uQ-BO06Y6l",
        "colab_type": "text"
      },
      "source": [
        "# Saving and Loading the Model\n",
        "In the script above, our machine learning model did not take much time to execute. One of the reasons for the quick training time is the fact that we had a relatively smaller training set. \n",
        "\n",
        "However, in real-world scenarios, there can be millions of documents. In such cases, it can take hours or even days to train the algorithms. Therefore, it is recommended to save the model once it is trained.\n",
        "\n",
        "We can save our model as a pickle object in Python. To do so, execute the following script:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5evA5xk6Y6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load Joblib to Save Model\n",
        "import joblib"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur8NHyjw6Y6n",
        "colab_type": "code",
        "outputId": "1a907036-ac2f-4c00-8218-968682150b64",
        "colab": {}
      },
      "source": [
        "# Save Linear Classifier on Word Level TF IDF Vectors\n",
        "save_models(linear_model.LogisticRegression(),xtrain_tfidf, train_y,\"logiticRegression_CountVectors\")\n",
        "\n",
        "# Save Linear Classifier on Ngram Level TF IDF Vectors\n",
        "save_models(linear_model.LogisticRegression(), xtrain_tfidf_ngram, train_y,\"logiticRegression_TF_IDF\")\n",
        "\n",
        "\n",
        "# Save Linear Classifier on Character Level TF IDF Vectors\n",
        "save_models(linear_model.LogisticRegression(), xtrain_tfidf_ngram_chars, train_y,\"logiticRegression_CharLevel\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved model as logiticRegression_CountVectors_model.pkl\n",
            "Saved model as logiticRegression_TF_IDF_model.pkl\n",
            "Saved model as logiticRegression_CharLevel_model.pkl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yH1KET__zqdW"
      },
      "source": [
        "#3.3 Implementing a SVM Model\n",
        "Support Vector Machine (SVM) is a supervised machine learning algorithm which is used for both classification or #regression challenges. \n",
        "The model extracts a best possible hyper-plane / line that segregates the two classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p6ogWzmKz0fT",
        "outputId": "df64e420-4e83-4ac6-903e-02c6eb34b681",
        "colab": {}
      },
      "source": [
        "# SVM on Ngram Level TF IDF Vectors\n",
        "accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram)\n",
        "print (\"SVM, N-Gram Vectors: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVM, N-Gram Vectors:  0.8733624454148472\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XsBIWvLy0Lht"
      },
      "source": [
        "#3.4 Bagging Model\n",
        "#Implementing a Random Forest Model\n",
        "\n",
        "Random Forest models are a type of ensemble models, particularly bagging models. They are part of the tree based #model family."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sGZg9rv10TLT",
        "outputId": "f2374eee-91d8-4d2d-e20d-db4e53b90ed3",
        "colab": {}
      },
      "source": [
        "# RF on Count Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_count, train_y, xvalid_count)\n",
        "print (\"RF, Count Vectors: \", accuracy)\n",
        "\n",
        "# RF on Word Level TF IDF Vectors\n",
        "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf)\n",
        "print (\"RF, WordLevel TF-IDF: \", accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RF, Count Vectors:  0.9082969432314411\n",
            "RF, WordLevel TF-IDF:  0.9082969432314411\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
            "/usr/local/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6tpmRTsi0bBW"
      },
      "source": [
        "#3.5 Boosting Model\n",
        "Implementing Xtereme Gradient Boosting Model\n",
        "\n",
        "Boosting models are another type of ensemble models part of tree based models. \n",
        "\n",
        "Boosting is a machine learning #ensemble meta-algorithm to reduce bias, and also variance in supervised learning, \n",
        "\n",
        "It is a family of #machine learning algorithms that convert weak learners to strong ones.\n",
        "\n",
        " A weak learner is defined to be a classifier #that is only slightly correlated with the true classification (it can label examples better than random guessing)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nDVYY8_50tsv",
        "outputId": "b94e60c1-1a1b-4ac3-e612-7f7d50ae210f",
        "colab": {}
      },
      "source": [
        "# Extereme Gradient Boosting on Count Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_count.tocsc(), train_y, xvalid_count.tocsc())\n",
        "print (\"Xgb, Count Vectors: \", accuracy)\n",
        "\n",
        "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc())\n",
        "print (\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
        "\n",
        "# Extereme Gradient Boosting on Character Level TF IDF Vectors\n",
        "accuracy = train_model(xgboost.XGBClassifier(), xtrain_tfidf_ngram_chars.tocsc(), train_y, xvalid_tfidf_ngram_chars.tocsc())\n",
        "print (\"Xgb, CharLevel Vectors: \", accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Xgb, Count Vectors:  0.8995633187772926\n",
            "Xgb, WordLevel TF-IDF:  0.8951965065502183\n",
            "Xgb, CharLevel Vectors:  0.8995633187772926\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PjHeVya6Y6v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip3 install -U scikit-learn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JRtZoj3j01f-"
      },
      "source": [
        "#3.6 Shallow Neural Networks\n",
        "Neural network is a mathematical model that is designed to behave similar to biological neurons and the nervous system. \n",
        "\n",
        "These models are used to recognize complex patterns and relationships that exists within a labelled data.\n",
        "\n",
        "The shallow neural network contains mainly three types of layers \n",
        "\n",
        " - Input layer\n",
        "\n",
        " - Hidden layer\n",
        " \n",
        " -Output layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X_PZJCgB1U9h",
        "outputId": "3097b217-ada2-4142-9b00-2fba341aa5a1",
        "colab": {}
      },
      "source": [
        "def create_model_architecture(input_size):\n",
        "    # create input layer \n",
        "    input_layer = layers.Input((input_size, ), sparse=True)\n",
        "    \n",
        "    # create hidden layer\n",
        "    hidden_layer = layers.Dense(100, activation=\"relu\")(input_layer)\n",
        "    \n",
        "    # create output layer\n",
        "    output_layer = layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
        "\n",
        "    classifier = models.Model(inputs = input_layer, outputs = output_layer)\n",
        "    classifier.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    return classifier \n",
        "\n",
        "classifier = create_model_architecture(xtrain_tfidf_ngram.shape[1])\n",
        "accuracy = train_model(classifier, xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, is_neural_net=True)\n",
        "print (\"NN, Ngram Level TF IDF Vectors\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0921 00:41:37.554676 4427470272 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0921 00:41:37.569904 4427470272 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:515: The name tf.sparse_placeholder is deprecated. Please use tf.compat.v1.sparse_placeholder instead.\n",
            "\n",
            "W0921 00:41:37.576919 4427470272 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0921 00:41:37.594110 4427470272 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1083: The name tf.sparse_tensor_dense_matmul is deprecated. Please use tf.sparse.sparse_dense_matmul instead.\n",
            "\n",
            "W0921 00:41:37.643635 4427470272 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0921 00:41:37.644861 4427470272 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0921 00:41:37.654021 4427470272 deprecation.py:323] From /Users/mazensalama/Library/Python/3.7/lib/python/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "534/534 [==============================] - 0s 523us/step - loss: 0.6429\n",
            "NN, Ngram Level TF IDF Vectors 0.12663755458515283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0qEwbcCUvEj9"
      },
      "source": [
        "#3.7 Deep Neural Networks\n",
        "\n",
        "Deep Neural Networks are more complex neural networks in which the hidden layers performs extrem and more complex #operations than the simple sigmoid or relu activations.\n",
        "\n",
        " Different types of deep learning models can be applied in text #classification problems."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ou50N-dKvEj-",
        "outputId": "5a9111ee-8eba-40f5-8c58-f7e71e4f2eb2",
        "colab": {}
      },
      "source": [
        "def create_cnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_cnn()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print (\"CNN, Word Embeddings\",  accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0921 00:41:38.315590 4427470272 deprecation.py:506] From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "534/534 [==============================] - 0s 790us/step - loss: 0.4585\n",
            "CNN, Word Embeddings 0.12663755458515283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_S0NiCgc17S8"
      },
      "source": [
        "#3.7.1 Convolutional Neural Network\n",
        "\n",
        "In the Convolutional neural networks, convolutions over the input layer are used to compute the output.\n",
        "\n",
        "This results in #local connections, where each region of the input is connected to a neuron in the output. Each layer applies #different filters and combines their results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "miIlvGKV2kTO",
        "outputId": "4966a8d4-599c-48e2-8165-fb3ccd36d198",
        "colab": {}
      },
      "source": [
        "def create_cnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_cnn()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print (\"CNN, Word Embeddings\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "534/534 [==============================] - 1s 1ms/step - loss: 0.4520\n",
            "CNN, Word Embeddings 0.12663755458515283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ufP1hqWmvEkC"
      },
      "source": [
        "#3.7.2 Recurrent Neural Network – LSTM\n",
        "\n",
        "Feed-forward neural networks in which activation outputs are propagated only in one direction, the activation #outputs from neurons propagate in both directions (from inputs to outputs and from outputs to inputs) in Recurrent #Neural Networks. Which creates loops in the neural network architecture which acts as a ‘memory state’ of the neurons. This state #allows the neurons an ability to remember what have been learned so far.\n",
        "\n",
        "The memory state in RNNs gives an advantage over traditional neural networks but a problem called Vanishing Gradient #is associated with them. In this problem, while learning with a large number of layers, it becomes really hard for #the network to learn and tune the parameters of the earlier layers. \n",
        "\n",
        "To address this problem, A new type of RNNs called LSTMs (Long Short Term Memory) Models have been developed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9b0VyhzrvEkC",
        "outputId": "d1221955-6d86-401f-8fd7-54cb132d02e7",
        "colab": {}
      },
      "source": [
        " def create_rnn_lstm():\n",
        "\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.LSTM(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rnn_lstm()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print (\"RNN-LSTM, Word Embeddings\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "534/534 [==============================] - 2s 3ms/step - loss: 0.4841\n",
            "RNN-LSTM, Word Embeddings 0.12663755458515283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x3ffIIkT3COK"
      },
      "source": [
        "#3.7.3 Recurrent Neural Network – GRU\n",
        "\n",
        "The Gated Recurrent Units are another form of recurrent neural networks.We will  add a layer of GRU instead of LSTM in our network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Bq2izQPn3LOr",
        "outputId": "2d610825-6181-473f-a413-173f870a95db",
        "colab": {}
      },
      "source": [
        "def create_rnn_gru():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the GRU Layer\n",
        "    lstm_layer = layers.GRU(100)(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rnn_gru()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print (\"RNN-GRU, Word Embeddings\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "534/534 [==============================] - 1s 3ms/step - loss: 0.5022\n",
            "RNN-GRU, Word Embeddings 0.12663755458515283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DAO7tpnSvEkG"
      },
      "source": [
        "#3.7.4 Bidirectional RNN\n",
        "RNN layers can be wrapped in Bidirectional layers as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zPEMl6kpvEkH",
        "outputId": "720b7218-fac9-43ac-9fdc-3ac2ecd3c33a",
        "colab": {}
      },
      "source": [
        "def create_bidirectional_rnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "\n",
        "    # Add the LSTM Layer\n",
        "    lstm_layer = layers.Bidirectional(layers.GRU(100))(embedding_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(lstm_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_bidirectional_rnn()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print (\"RNN-Bidirectional, Word Embeddings\",  accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "534/534 [==============================] - 2s 4ms/step - loss: 0.5174\n",
            "RNN-Bidirectional, Word Embeddings 0.12663755458515283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bDVRbHTjvEkI"
      },
      "source": [
        "#3.7.5 Recurrent Convolutional Neural Network\n",
        "\n",
        "As the essential architectures have been tried out, We can try different variants of these layers such as recurrent convolutional neural network. Another variants can be:\n",
        "\n",
        "-Hierarichial Attention Networks\n",
        "\n",
        "-Sequence to Sequence Models with Attention\n",
        "\n",
        "-Bidirectional Recurrent Convolutional Neural Networks\n",
        "\n",
        "-CNNs and RNNs with more number of layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MiLf249UvEkJ",
        "outputId": "fe9ab878-e466-495c-b34e-6892ab5c1d5f",
        "colab": {}
      },
      "source": [
        "def create_rcnn():\n",
        "    # Add an Input Layer\n",
        "    input_layer = layers.Input((70, ))\n",
        "\n",
        "    # Add the word embedding Layer\n",
        "    embedding_layer = layers.Embedding(len(word_index) + 1, 300, weights=[embedding_matrix], trainable=False)(input_layer)\n",
        "    embedding_layer = layers.SpatialDropout1D(0.3)(embedding_layer)\n",
        "    \n",
        "    # Add the recurrent layer\n",
        "    rnn_layer = layers.Bidirectional(layers.GRU(50, return_sequences=True))(embedding_layer)\n",
        "    \n",
        "    # Add the convolutional Layer\n",
        "    conv_layer = layers.Convolution1D(100, 3, activation=\"relu\")(embedding_layer)\n",
        "\n",
        "    # Add the pooling Layer\n",
        "    pooling_layer = layers.GlobalMaxPool1D()(conv_layer)\n",
        "\n",
        "    # Add the output Layers\n",
        "    output_layer1 = layers.Dense(50, activation=\"relu\")(pooling_layer)\n",
        "    output_layer1 = layers.Dropout(0.25)(output_layer1)\n",
        "    output_layer2 = layers.Dense(1, activation=\"sigmoid\")(output_layer1)\n",
        "\n",
        "    # Compile the model\n",
        "    model = models.Model(inputs=input_layer, outputs=output_layer2)\n",
        "    model.compile(optimizer=optimizers.Adam(), loss='binary_crossentropy')\n",
        "    \n",
        "    return model\n",
        "\n",
        "classifier = create_rcnn()\n",
        "accuracy = train_model(classifier, train_seq_x, train_y, valid_seq_x, is_neural_net=True)\n",
        "print (\"CNN, Word Embeddings\",  accuracy)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1\n",
            "534/534 [==============================] - 1s 2ms/step - loss: 0.4335\n",
            "CNN, Word Embeddings 0.12663755458515283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZC8CqXU6Y7B",
        "colab_type": "text"
      },
      "source": [
        "# The diagnostic measures covered  for High Accuracutre ML Model are:\n",
        "\n",
        "1) accuracy: proportion of test results that are correct\n",
        "\n",
        "2) sensitivity: proportion of true +ve identified\n",
        "\n",
        "3) specificity: proportion of true -ve identified\n",
        "\n",
        "4) positive likelihood: increased probability of true +ve if test +ve\n",
        "\n",
        "5) negative likelihood: reduced probability of true +ve if test -ve\n",
        "\n",
        "6) false positive rate: proportion of false +ves in true -ve patients\n",
        "\n",
        "7) false negative rate: proportion of false -ves in true +ve patients\n",
        "\n",
        "8) positive predictive value: chance of true +ve if test +ve\n",
        "\n",
        "9) negative predictive value: chance of true -ve if test -ve\n",
        "\n",
        "10) precision = positive predictive value\n",
        "\n",
        "11) recall = sensitivity\n",
        "\n",
        "12) f1 = (2 * precision * recall) / (precision + recall)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bJXyTdj6Y7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "def calculate_diagnostic_performance (actual_predicted):\n",
        "    \"\"\" Calculate diagnostic performance.\n",
        "    \n",
        "    Takes a Numpy array of 1 and zero, two columns: actual and predicted\n",
        "    \n",
        "    Note that some statistics are repeats with different names\n",
        "    (precision = positive_predictive_value and recall = sensitivity).\n",
        "    Both names are returned\n",
        "    \n",
        "    Returns a dictionary of results:\n",
        "        \n",
        "    1) accuracy: proportion of test results that are correct    \n",
        "    2) sensitivity: proportion of true +ve identified\n",
        "    3) specificity: proportion of true -ve identified\n",
        "    4) positive likelihood: increased probability of true +ve if test +ve\n",
        "    5) negative likelihood: reduced probability of true +ve if test -ve\n",
        "    6) false positive rate: proportion of false +ves in true -ve patients\n",
        "    7) false negative rate:  proportion of false -ves in true +ve patients\n",
        "    8) positive predictive value: chance of true +ve if test +ve\n",
        "    9) negative predictive value: chance of true -ve if test -ve\n",
        "    10) precision = positive predictive value \n",
        "    11) recall = sensitivity\n",
        "    12) f1 = (2 * precision * recall) / (precision + recall)\n",
        "    13) positive rate = rate of true +ve (not strictly a performance measure)\n",
        "    \"\"\"\n",
        "    y = actual_predicted[:, 0]\n",
        "    y_pred = actual_predicted[:, 1]\n",
        "    CM = metrics.confusion_matrix(y, y_pred)\n",
        "    TN=CM[0][0]\n",
        "    FN=CM[1][0]\n",
        "    FP=CM[0][1]\n",
        "    TP=CM[1][1]\n",
        "    accuracy = (TP+TN)*1.0/(TP+TN+FP+FN)\n",
        "    PPV = TP*1.0/(TP+FP)\n",
        "    NPV = TN*1.0/(TN+FN)\n",
        "    sensitivity = TP*1.0/(TP+FN)\n",
        "    specificity = TN*1.0/(FP+TN)\n",
        "    fscore = 2.0*((sensitivity*PPV)/(sensitivity + PPV))\n",
        "\n",
        "    # Calculate results\n",
        "    positive_likelihood = sensitivity / (1 - specificity)\n",
        "    negative_likelihood = (1 - sensitivity) / specificity\n",
        "\n",
        "    false_positive_rate = 1 - specificity\n",
        "    false_negative_rate = 1 - sensitivity\n",
        "\n",
        "    precision = PPV\n",
        "    recall = sensitivity\n",
        "\n",
        "    positive_rate = np.mean(actual_predicted[:,1])\n",
        "\n",
        "    # Add results to dictionary\n",
        "    performance = {}\n",
        "    performance['accuracy'] = accuracy\n",
        "    performance['sensitivity'] = sensitivity\n",
        "    performance['specificity'] = specificity\n",
        "    performance['positive_likelihood'] = positive_likelihood\n",
        "    performance['negative_likelihood'] = negative_likelihood\n",
        "    performance['false_positive_rate'] = false_positive_rate\n",
        "    performance['false_negative_rate'] = false_negative_rate\n",
        "    performance['positive_predictive_value'] = PPV\n",
        "    performance['negative_predictive_value'] = NPV\n",
        "    performance['precision'] = precision\n",
        "    performance['recall'] = recall\n",
        "    performance['f1'] = fscore\n",
        "    performance['positive_rate'] = positive_rate\n",
        "\n",
        "    return performance"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfb5WXTp6Y7D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_performance(y, y_pred):\n",
        "    CM = metrics.confusion_matrix(y, y_pred)\n",
        "    TN=CM[0][0]\n",
        "    FN=CM[1][0]\n",
        "    FP=CM[0][1]\n",
        "    TP=CM[1][1]\n",
        "    accuracy = (TP+TN)*1.0/(TP+TN+FP+FN)\n",
        "    PPV = TP*1.0/(TP+FP)\n",
        "    NPV = TN*1.0/(TN+FN)\n",
        "    sensitivity = TP*1.0/(TP+FN)\n",
        "    specificity = TN*1.0/(FP+TN)\n",
        "    fscore = 2.0*((sensitivity*PPV)/(sensitivity + PPV))\n",
        "    return [accuracy, PPV, NPV, sensitivity, specificity, fscore]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpJ4zqpd6Y7F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_vocab(vectorizer, vector, parameter_name, string):\n",
        "    scores = zip(vectorizer.get_feature_names(), np.asarray(vector.sum(axis=0)).ravel())\n",
        "    sorted_scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
        "    with open('./vocab/' + parameter_name + '_feat' + string + '.csv', 'w') as csvfile:\n",
        "        fwriter = csv.writer(csvfile)\n",
        "        for x in sorted_scores:\n",
        "            fwriter.writerow(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3eC1k9-6Y7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### How To Load Saved Model\n",
        "# Load the model from the file \n",
        "#model_from_joblib = joblib.load('model_filename.pkl')  \n",
        "# Use the loaded model to make predictions \n",
        "#model_from_joblib.predict(X_test) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDKwRc7M6Y7K",
        "colab_type": "text"
      },
      "source": [
        "### Diagnostic Performance Simplified\n",
        "#### This is an Example using Pandas_ml\n",
        "#### install pandas-ml with\n",
        "#### pip install pandas-ml\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doHhD-ox6Y7L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Install Pandas-ml\n",
        "#!pip install pandas-ml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPZehkoz6Y7M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using Pandas ML\n",
        "from pandas_ml import ConfusionMatrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hiNKWi-d6Y7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to Check For Performance\n",
        "def check_model_performance(classifier, feature_vector_train, label, feature_vector_test,label_test):\n",
        "    # fit the training dataset on the classifier\n",
        "    classifier.fit(feature_vector_train, label)\n",
        "    \n",
        "    # predict the labels on validation dataset\n",
        "    y_predictions = classifier.predict(feature_vector_test)\n",
        "    our_confusion_matrix = ConfusionMatrix(label_test,y_predictions)\n",
        "    print(our_confusion_matrix.print_stats())\n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0LTgDig6Y7Q",
        "colab_type": "code",
        "outputId": "60d2ee2f-cad3-400f-af60-3de8582dba26",
        "colab": {}
      },
      "source": [
        "# In Your Case it would be like this For the Previous Model you saved\n",
        "# For Linear Classifier on Count Vectors\n",
        "check_model_performance(linear_model.LogisticRegression(), xtrain_count, train_y, xvalid_count,valid_y)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "population: 229\n",
            "P: 200\n",
            "N: 29\n",
            "PositiveTest: 204\n",
            "NegativeTest: 25\n",
            "TP: 192\n",
            "TN: 17\n",
            "FP: 12\n",
            "FN: 8\n",
            "TPR: 0.96\n",
            "TNR: 0.5862068965517241\n",
            "PPV: 0.9411764705882353\n",
            "NPV: 0.68\n",
            "FPR: 0.41379310344827586\n",
            "FDR: 0.058823529411764705\n",
            "FNR: 0.04\n",
            "ACC: 0.9126637554585153\n",
            "F1_score: 0.9504950495049505\n",
            "MCC: 0.5824868000315143\n",
            "informedness: 0.5462068965517242\n",
            "markedness: 0.6211764705882352\n",
            "prevalence: 0.8733624454148472\n",
            "LRP: 2.32\n",
            "LRN: 0.06823529411764706\n",
            "DOR: 34.0\n",
            "FOR: 0.32\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuzouQ6U8iOc",
        "colab_type": "text"
      },
      "source": [
        "Definition of the Terms:\n",
        "Class Statistics:\n",
        "\n",
        "Classes                                       \n",
        "Population                                    \n",
        "P: Condition positive                          \n",
        "N: Condition negative                          \n",
        "Test outcome positive                          \n",
        "Test outcome negative                          \n",
        "TP: True Positive                              \n",
        "TN: True Negative                              \n",
        "FP: False Positive                             \n",
        "FN: False Negative                             \n",
        "TPR: (Sensitivity, hit rate, recall)           \n",
        "TNR=SPC: (Specificity)                 \n",
        "PPV: Pos Pred Value (Precision)              \n",
        "NPV: Neg Pred Value                            \n",
        "FPR: False-out                         \n",
        "FDR: False Discovery Rate                    \n",
        "FNR: Miss Rate                                 \n",
        "ACC: Accuracy                          \n",
        "F1 score                                    \n",
        "MCC: Matthews correlation coefficient  \n",
        "Informedness                           \n",
        "Markedness                                   \n",
        "Prevalence                                  \n",
        "LR+: Positive likelihood ratio               \n",
        "LR-: Negative likelihood ratio                 \n",
        "DOR: Diagnostic odds ratio                   \n",
        "FOR: False omission rate                       "
      ]
    }
  ]
}